{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation of Different Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import scipy\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from src.models import classification_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_transformer = classification_transformer.Main(\"large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>artist</th>\n",
              "      <th>decade</th>\n",
              "      <th>genre</th>\n",
              "      <th>ratings</th>\n",
              "      <th>stars</th>\n",
              "      <th>chords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://tabs.ultimate-guitar.com/tab/jeff-buck...</td>\n",
              "      <td>Hallelujah</td>\n",
              "      <td>Jeff Buckley</td>\n",
              "      <td>1990</td>\n",
              "      <td>Rock|Folk</td>\n",
              "      <td>51639.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[tensor(648), tensor(640), tensor(648), tensor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://tabs.ultimate-guitar.com/tab/ed-sheera...</td>\n",
              "      <td>Perfect</td>\n",
              "      <td>Ed Sheeran</td>\n",
              "      <td>2010</td>\n",
              "      <td>Pop</td>\n",
              "      <td>44194.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[tensor(155), tensor(31), tensor(648), tensor(...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://tabs.ultimate-guitar.com/tab/elvis-pre...</td>\n",
              "      <td>Cant Help Falling In Love</td>\n",
              "      <td>Elvis Presley</td>\n",
              "      <td>1960</td>\n",
              "      <td>Soundtrack|R&amp;B, Funk &amp; Soul</td>\n",
              "      <td>30059.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[tensor(648), tensor(155), tensor(640), tensor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://tabs.ultimate-guitar.com/tab/eagles/ho...</td>\n",
              "      <td>Hotel California</td>\n",
              "      <td>Eagles</td>\n",
              "      <td>1970</td>\n",
              "      <td>Rock</td>\n",
              "      <td>28670.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[tensor(166), tensor(403), tensor(380), tensor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://tabs.ultimate-guitar.com/tab/radiohead...</td>\n",
              "      <td>Creep</td>\n",
              "      <td>Radiohead</td>\n",
              "      <td>1990</td>\n",
              "      <td>Rock</td>\n",
              "      <td>28606.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[tensor(155), tensor(78), tensor(648), tensor(...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 url  \\\n",
              "0  https://tabs.ultimate-guitar.com/tab/jeff-buck...   \n",
              "1  https://tabs.ultimate-guitar.com/tab/ed-sheera...   \n",
              "2  https://tabs.ultimate-guitar.com/tab/elvis-pre...   \n",
              "3  https://tabs.ultimate-guitar.com/tab/eagles/ho...   \n",
              "4  https://tabs.ultimate-guitar.com/tab/radiohead...   \n",
              "\n",
              "                       title         artist  decade  \\\n",
              "0                 Hallelujah   Jeff Buckley    1990   \n",
              "1                    Perfect     Ed Sheeran    2010   \n",
              "2  Cant Help Falling In Love  Elvis Presley    1960   \n",
              "3           Hotel California         Eagles    1970   \n",
              "4                      Creep      Radiohead    1990   \n",
              "\n",
              "                         genre  ratings  stars  \\\n",
              "0                    Rock|Folk  51639.0    5.0   \n",
              "1                          Pop  44194.0    5.0   \n",
              "2  Soundtrack|R&B, Funk & Soul  30059.0    5.0   \n",
              "3                         Rock  28670.0    5.0   \n",
              "4                         Rock  28606.0    5.0   \n",
              "\n",
              "                                              chords  \n",
              "0  [tensor(648), tensor(640), tensor(648), tensor...  \n",
              "1  [tensor(155), tensor(31), tensor(648), tensor(...  \n",
              "2  [tensor(648), tensor(155), tensor(640), tensor...  \n",
              "3  [tensor(166), tensor(403), tensor(380), tensor...  \n",
              "4  [tensor(155), tensor(78), tensor(648), tensor(...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('../Data/data_tokenized_pitch_class.csv')\n",
        "data[\"chords\"] = data[\"chords\"].apply(lambda x: torch.tensor(json.loads(x)))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into train and test sets as in the previous notebooks\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class ChordDataset: # A dummy dataset class\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "dataset = ChordDataset(data)\n",
        "train_size = int(np.rint(len(dataset) * 0.8))\n",
        "train_indices, test_indices = random_split(range(len(dataset)), [train_size, len(dataset) - train_size])\n",
        "\n",
        "# Convert the indices to lists\n",
        "train_indices = [idx for idx in train_indices.indices]\n",
        "test_indices = [idx for idx in test_indices.indices]\n",
        "\n",
        "# Split the dataframe using the indices\n",
        "train_data = data.iloc[train_indices]\n",
        "test_data = data.iloc[test_indices]\n",
        "\n",
        "# Reindex the dataframes\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "test_data = test_data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 17792, Test size: 4448\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train size: {len(train_data)}, Test size: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tester"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Evaluation:\n",
        "    def __init__(self, ref_column):\n",
        "        self.ref_column = ref_column        \n",
        "        self.augmentation_map = torch.tensor(np.load('../Data/augmentation_map.npy', allow_pickle=True))\n",
        "        self.augmentation_map = self.augmentation_map.to(embedding_transformer.device)\n",
        "\n",
        "    \n",
        "    def augment(self, chords):\n",
        "        \"\"\"Change the root note of the chords by a random amount\"\"\"\n",
        "        move_by = torch.randint(0, 12, [1]).item()\n",
        "        return self.augmentation_map[chords, move_by]\n",
        "        \n",
        "    def pad(self, chords):\n",
        "        \"\"\"Pad the input 2D tensor [n] into shape [256] with zeros\"\"\"\n",
        "        out = torch.zeros((256), dtype=torch.long, device=embedding_transformer.device)\n",
        "        out[:len(chords)] = chords\n",
        "        return out\n",
        "    \n",
        "    def reduce_dimensionality(self, column, batch_size=64, augment=False):\n",
        "        \"\"\"Use a pretrained classifier to reduce the dimensionality of the dataframes.\"\"\"\n",
        "        \n",
        "        reduced_column = []\n",
        "        n = len(column)\n",
        "        \n",
        "        for i in range(0, n, batch_size):\n",
        "            # Extract the current batch of data\n",
        "            batch = column[i:i+batch_size]\n",
        "            \n",
        "            # Convert batch to torch tensor and process it\n",
        "            batch_tensor = [torch.tensor(item.tolist(), dtype=torch.long, device=embedding_transformer.device) for item in batch]\n",
        "            if augment:\n",
        "                batch_tensor = torch.stack([self.pad(self.augment(item)) for item in batch_tensor])\n",
        "            else:\n",
        "                batch_tensor = torch.stack([self.pad(item) for item in batch_tensor])\n",
        "            \n",
        "            # Get embeddings for the entire batch and append to reduced_column\n",
        "            batch_embeddings = embedding_transformer.batch_extract_features(batch_tensor)\n",
        "            reduced_column.extend(batch_embeddings)\n",
        "            \n",
        "        return reduced_column\n",
        "    \n",
        "    def frechet_distance(self, mu1, mu2, sigma1, sigma2):\n",
        "        \"\"\"\n",
        "        Compute the Frechet distance between two multivariate Gaussians.\n",
        "        \n",
        "        Args:\n",
        "            mu1, mu2: mean vectors (1D numpy arrays)\n",
        "            sigma1, sigma2: covariance matrices (2D numpy arrays)\n",
        "            \n",
        "        Returns:\n",
        "            The Frechet distance between the two distributions.\n",
        "        \"\"\"\n",
        "        mu_diff = mu1 - mu2\n",
        "        # The following line computes (sigma1 * sigma2)^(1/2) using the matrix square root\n",
        "        sqrt_sigma = scipy.linalg.sqrtm(np.dot(sigma1, sigma2))\n",
        "        \n",
        "        # Handling numerical instability (may occur if matrices are nearly singular)\n",
        "        if not np.isfinite(sqrt_sigma).all():\n",
        "            offset = np.eye(sigma1.shape[0]) * 1e-10\n",
        "            sqrt_sigma = scipy.linalg.sqrtm(np.dot(sigma1 + offset, sigma2 + offset))\n",
        "        \n",
        "        # Compute the trace term\n",
        "        tr_term = np.trace(sigma1 + sigma2 - 2 * sqrt_sigma)\n",
        "        \n",
        "        # Compute the difference term\n",
        "        diff_term = np.dot(mu_diff, mu_diff)\n",
        "        \n",
        "        return diff_term + tr_term\n",
        "    \n",
        "    def calculate_frechet_distance(self, ref_column, gen_column):\n",
        "        \"\"\"Calculate the Frechet distance between the reference and generated samples for the reduced columns.\"\"\"\n",
        "        ref_column, gen_column = np.array(ref_column), np.array(gen_column)\n",
        "        mu1 = np.mean(ref_column, axis=0)\n",
        "        mu2 = np.mean(gen_column, axis=0)\n",
        "\n",
        "        sigma1 = np.cov(ref_column, rowvar=False)\n",
        "        sigma2 = np.cov(gen_column, rowvar=False)\n",
        "\n",
        "        # Compute the Frechet distance\n",
        "        return self.frechet_distance(mu1, mu2, sigma1, sigma2)\n",
        "    \n",
        "    def preprocess_ref_col(self):\n",
        "        \"\"\"Reduce the dimensionality of the reference column.\"\"\"\n",
        "        self.ref_reduced_column = self.reduce_dimensionality(self.ref_column, augment=True)\n",
        "    \n",
        "    def get_column_score(self, gen_column):\n",
        "        \"\"\"Get the score for a generated column.\"\"\"\n",
        "        gen_reduced_column = self.reduce_dimensionality(gen_column)\n",
        "        distances = self.calculate_frechet_distance(self.ref_reduced_column, gen_reduced_column)\n",
        "        return distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
            "  return torch._native_multi_head_attention(\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "eval = Evaluation(test_data[\"chords\"])\n",
        "eval.preprocess_ref_col()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(file_name):\n",
        "    gen_df = pd.read_csv(f\"../Data/Generated/{file_name}.csv\")\n",
        "    gen_df[\"chords\"] = gen_df[\"chords\"].apply(lambda x: torch.tensor(json.loads(x), dtype=torch.long))\n",
        "    score = eval.get_column_score(gen_df[\"chords\"])\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
            "  return torch._native_multi_head_attention(\n",
            "d:\\Programs\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
            "  return torch._native_multi_head_attention(\n",
            "d:\\Programs\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
            "  return torch._native_multi_head_attention(\n",
            "d:\\Programs\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
            "  return torch._native_multi_head_attention(\n",
            "d:\\Programs\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
            "  return torch._native_multi_head_attention(\n",
            "d:\\Programs\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
            "  return torch._native_multi_head_attention(\n",
            "d:\\Programs\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
            "  return torch._native_multi_head_attention(\n",
            "d:\\Programs\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
            "  return torch._native_multi_head_attention(\n",
            "d:\\Programs\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
            "  return torch._native_multi_head_attention(\n",
            "d:\\Programs\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
            "  return torch._native_multi_head_attention(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>recurrent_net</td>\n",
              "      <td>9.474979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>transformer_small</td>\n",
              "      <td>3.882284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>transformer_medium</td>\n",
              "      <td>2.549732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>transformer_large</td>\n",
              "      <td>2.021196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>conditional_small</td>\n",
              "      <td>3.934798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>conditional_medium</td>\n",
              "      <td>1.935170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>conditional_large</td>\n",
              "      <td>1.424958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>style_small</td>\n",
              "      <td>2.189804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>style_medium</td>\n",
              "      <td>0.970757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>style_large</td>\n",
              "      <td>0.705609</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                model     score\n",
              "0       recurrent_net  9.474979\n",
              "1   transformer_small  3.882284\n",
              "2  transformer_medium  2.549732\n",
              "3   transformer_large  2.021196\n",
              "4   conditional_small  3.934798\n",
              "5  conditional_medium  1.935170\n",
              "6   conditional_large  1.424958\n",
              "7         style_small  2.189804\n",
              "8        style_medium  0.970757\n",
              "9         style_large  0.705609"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = []\n",
        "sizes = [\"small\", \"medium\", \"large\"]\n",
        "files = [\"recurrent_net\"] + [f\"transformer_{s}\" for s in sizes] + [f\"conditional_{s}\" for s in sizes] + [f\"style_{s}\" for s in sizes]\n",
        "for file_name in files:\n",
        "    scores.append({\"model\": file_name, \"score\": evaluate(file_name)})\n",
        "scores = pd.DataFrame(scores)\n",
        "scores"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
